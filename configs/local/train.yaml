# Full Training: TCN + Bi-Mamba + GNN Stack (v2 or v3)
# Optimized for RTX 4090 (24GB VRAM) with NaN protections
# Based on EvoBrain proven parameters + RTX 4090 best practices
#
# IMPORTANT:
# - GPU stack: run `make setup-gpu` (installs Mamba+PyG with prebuilt wheels)
# - Ensure cache exists at cache/tusz with 3734 train + 933 val NPZ (see configs/README.md for cache build instructions)
# - Monitor for NaNs: export BGB_NAN_DEBUG=1 BGB_SANITIZE_INPUTS=1
# - If local training stalls/hangs (esp. on WSL2): set data.num_workers: 0
# - If NaNs persist on RTX 4090: set training.mixed_precision: false; reduce batch_size or learning_rate

experiment:
  name: full_training
  description: "Full training - TCN+BiMamba+GNN on RTX 4090"
  seed: 42
  output_dir: results/full_training
  cache_dir: cache/full_training
  device: cuda
  log_level: INFO
  save_model: true
  save_best_only: true

  # W&B configuration (optional)
  wandb:
    enabled: false
    project: seizure-v3
    entity: null

model:
  # Architecture: "tcn" (v2) or "v3" (dual-stream with edge Mamba)
  architecture: v3  # V3 dual-stream with learned adjacency

  tcn:
    num_layers: 8
    kernel_size: 7
    dropout: 0.15
    causal: false
    stride_down: 16
    use_cuda_optimizations: true

  mamba:
    n_layers: 6
    d_model: 512
    d_state: 16
    conv_kernel: 4  # CUDA constraint: must be 2-4
    dropout: 0.1

  # Graph configuration (works for both v2 and v3)
  graph:
    enabled: true              # Required for both v2 and v3

    # V3: Edge stream config
    edge_features: cosine      # Edge feature metric for v3
    edge_top_k: 3              # Top-k edges per node for v3
    edge_threshold: 1.0e-4     # Edge weight cutoff for v3
    edge_mamba_layers: 2       # Edge Mamba layers for v3
    edge_mamba_d_state: 8      # Edge Mamba state dim for v3
    edge_mamba_d_model: 16     # Edge Mamba model dim (learned lift 1→16→1)

    # V2 heuristic removed

    # Shared GNN architecture (both v2.6 and v3)
    n_layers: 2                # 2-layer GNN
    dropout: 0.1
    use_residual: true         # Skip connections
    alpha: 0.05                # SSGConv mixing
    k_eigenvectors: 16         # Laplacian PE dimension

    # Dynamic PE config (v3) - OPTIMIZED FOR RTX 4090
    use_dynamic_pe: true       # RE-ENABLED after fixing edge NaN issues
    semi_dynamic_interval: 5   # OPTIMAL: PE every 19.5ms (192 eigendecomps)
    pe_sign_consistency: true  # Fix eigenvector signs for stability

  # Decoder removed in V3-only architecture

data:
  dataset: tuh_eeg
  data_dir: data_ext4/tusz/edf        # Parent dir containing train/dev/eval
  cache_dir: cache/tusz                # Will create train/ and dev/ subdirs
  split_policy: official_tusz          # Use TUSZ official patient-disjoint splits!
  sampling_rate: 256
  n_channels: 19
  window_size: 60
  stride: 10
  use_balanced_sampling: true         # CRITICAL for imbalanced data
  num_workers: 0                      # WSL2 fix: prevents multiprocessing hangs
  pin_memory: true                    # Faster GPU transfer
  persistent_workers: true            # Reuse workers
  prefetch_factor: 2                  # Pre-load batches
  validation_split: 0.2

preprocessing:
  montage: "10-20"
  bandpass: [0.5, 120.0]
  notch_freq: 60
  normalize: true
  use_mne: true

training:
  epochs: 100

  # RTX 4090 OPTIMAL: balanced memory/speed/safety
  # 4 = sweet spot: 16GB usage, 8GB buffer, 33% faster than size=3
  batch_size: 4   # SAFE: uses 16GB out of 24GB

  # Ultra-conservative learning rate after epoch 25 NaN explosion
  # RTX 4090 best practice: start very low for stability
  learning_rate: 1.0e-5  # Further reduced from 5.0e-5 after epoch 25 explosion
  weight_decay: 0.01     # Reduced from 0.05 to prevent weight explosion
  optimizer: adamw

  # Stronger RTX 4090 NaN protections after epoch 25 failure
  gradient_clip: 0.1      # Much stronger clipping (was 0.5)

  # Mixed precision OFF by default for RTX 4090 stability
  # Only enable after confirming no NaNs with a smoke test
  mixed_precision: false  # Set true only after stable training confirmed

  # Alternative if FP16 causes issues (requires PyTorch 2.0+):
  # mixed_precision_dtype: bfloat16  # More stable than fp16

  # Focal loss for extreme class imbalance
  loss: focal
  focal_alpha: 0.5       # Neutral alpha (let pos_weight handle imbalance)
  focal_gamma: 2.0       # Focus on hard examples

  # Learning rate schedule - minimal decay after epoch 25 NaN issue
  scheduler:
    type: cosine  # Back to cosine but with very conservative LR
    warmup_ratio: 0.10   # 10% warmup for smoother start on RTX 4090

  # Early stopping
  early_stopping:
    patience: 5
    metric: sensitivity_at_10fa

  # Checkpointing
  checkpoint_interval: 1  # Save every epoch for safety
  gradient_accumulation_steps: 1  # Can increase if batch_size too small

postprocessing:
  hysteresis:
    tau_on: 0.86
    tau_off: 0.78
  morphology:
    opening_kernel: 11
    closing_kernel: 31
  duration:
    min_duration_s: 3.0
    max_duration_s: 600.0
  events:
    tau_merge: 2.0
    confidence_method: mean

evaluation:
  metrics: [taes, sensitivity, specificity, auroc]
  fa_rates: [10, 5, 2.5, 1]
  save_predictions: false
  save_plots: false

logging:
  log_every_n_steps: 50
  log_gradients: false
  log_weights: false

# Debug environment variables (set these for troubleshooting):
# export BGB_NAN_DEBUG=1          # Enable NaN debugging
# export BGB_SANITIZE_INPUTS=1    # Replace NaN/Inf with zeros
# export BGB_ANOMALY_DETECT=1     # PyTorch anomaly detection (slow!)
# export BGB_DISABLE_TQDM=0       # Keep progress bars locally

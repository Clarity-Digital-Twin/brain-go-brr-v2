# TUSZ training configuration - WSL2-safe version
# Prevents OOM and multiprocessing issues on local WSL2 systems
#
# IMPORTANT GPU REQUIREMENTS:
# - Minimum: NVIDIA RTX 4090 or equivalent (24GB VRAM)
# - CPU-only training is impractical (~40s/batch vs ~0.5s/batch on 4090)
# - Estimated training time for 100 epochs on RTX 4090: ~16-20 hours
# - Estimated training time on CPU: ~4 years (NOT RECOMMENDED)

data:
  dataset: tuh_eeg
  data_dir: data_ext4/tusz/edf/train   # FAST ext4! root containing .edf + .csv pairs
  cache_dir: cache/tcn_full             # TCN-specific cache
  sampling_rate: 256
  n_channels: 19
  window_size: 60
  stride: 10
  num_workers: 4                        # Parallel data loading (tested OK on WSL2)
  pin_memory: true                       # Faster GPU transfers
  persistent_workers: true               # Reuse workers between epochs
  prefetch_factor: 2                     # Pre-load 2 batches per worker
  validation_split: 0.2
  use_balanced_sampling: true          # CRITICAL: Use positive-aware sampling

preprocessing:
  montage: "10-20"
  bandpass: [0.5, 120]
  notch_freq: 60
  normalize: true
  use_mne: true

model:
  architecture: tcn  # Use TCN instead of U-Net

  tcn:
    num_layers: 8
    channels: [64, 128, 256, 512]
    kernel_size: 7
    dropout: 0.15
    causal: false
    stride_down: 16
    use_cuda_optimizations: true
  mamba:
    n_layers: 6
    d_model: 512
    d_state: 16
    conv_kernel: 4
    dropout: 0.1
  decoder:
    stages: 4
    kernel_size: 4

postprocessing:
  hysteresis:
    tau_on: 0.86
    tau_off: 0.78
  morphology:
    opening_kernel: 11
    closing_kernel: 31
  duration:
    min_duration_s: 3.0
    max_duration_s: 600.0
  events:
    tau_merge: 2.0
    confidence_method: mean

training:
  epochs: 100                          # full training - will early stop if converged
  batch_size: 8                        # WSL2-safe: reduced from 16 to prevent OOM
  loss: focal                          # USE FOCAL LOSS FOR EXTREME IMBALANCE!
  focal_alpha: 0.5                     # Neutral alpha; rely on pos_weight from dataset sample
  focal_gamma: 2.0                     # Focusing parameter (higher = more focus on hard examples)
  learning_rate: 3e-4
  weight_decay: 0.05
  optimizer: adamw
  scheduler:
    type: cosine
    warmup_ratio: 0.03
  gradient_clip: 1.0
  mixed_precision: true
  early_stopping:
    patience: 5
    metric: sensitivity_at_10fa
  checkpoint_interval: 5               # Save every 5 epochs for safety

evaluation:
  metrics: [taes, sensitivity, specificity, auroc]
  fa_rates: [10, 5, 2.5, 1]
  save_predictions: false
  save_plots: false

experiment:
  name: tcn_full_100ep
  description: "TCN architecture full training 100 epochs"
  seed: 42
  device: cuda  # REQUIRED: Must use CUDA
  output_dir: results/tcn_full_100ep
  cache_dir: cache/tcn_full
  log_level: INFO
  save_model: true
  save_best_only: true

logging:
  log_every_n_steps: 50
  log_gradients: false
  log_weights: false

# Brain-Go-Brr V3 Configuration Files

## üß† Architecture: V3 Dual-Stream (TCN + BiMamba + GNN)

All configs use the V3 dual-stream architecture:
- **TCN**: Multi-scale temporal feature extraction (8 layers, stride=16)
- **Node Stream**: Per-electrode BiMamba (d_model=64, 6 layers, headdim=8)
- **Edge Stream**: Per-edge BiMamba (d_model=16, 2 layers, headdim=4)
- **GNN**: Vectorized SSGConv with **DYNAMIC** Laplacian PE (Œ±=0.05, k=16)
  - **Dynamic PE**: Recomputed per timestep from evolving adjacency (EvoBrain approach)
  - **Vectorized**: All 960 timesteps computed in parallel (100-1000x faster than loops)
  - **Numerical Stability**: FP32 eigendecomposition with sign consistency
- **Total Parameters**: ~31.5M

## üìÅ Directory Structure

```
configs/
‚îú‚îÄ‚îÄ local/                    # Local WSL2/Linux configs (RTX 4090 optimized)
‚îÇ   ‚îú‚îÄ‚îÄ smoke.yaml           # Quick test (1 epoch, 3 files via BGB_LIMIT_FILES=3)
‚îÇ   ‚îî‚îÄ‚îÄ train.yaml           # Full training (100 epochs, 3734 files)
‚îÇ
‚îî‚îÄ‚îÄ modal/                    # Modal cloud GPU configs (A100-80GB optimized)
    ‚îú‚îÄ‚îÄ smoke.yaml           # Quick cloud test (1 epoch, 50 files)
    ‚îî‚îÄ‚îÄ train.yaml           # Full cloud training (100 epochs, 3734 files)
```

## ‚ö° Critical Cache Configuration

### Local (RTX 4090)
```yaml
data:
  cache_dir: cache/tusz     # MUST use existing cache with 3734 files!
```
- **Location**: `cache/tusz/train/` (3734 NPZ) + `cache/tusz/val/` (933 NPZ)
- **Warning**: Do NOT use `cache/v2.6_full/` - it's empty!

### Modal (A100)
```yaml
data:
  cache_dir: /results/cache/tusz  # Persistent SSD volume
```
- **Location**: `/results/cache/tusz/train/` + `/results/cache/tusz/val/`
- **Built once**: First run builds cache, all subsequent runs reuse
- **NOT on S3**: Cache is on fast Modal SSD, never touches S3 after build

## üöÄ Usage Examples

### Local Training (RTX 4090)
```bash
# Smoke test (requires environment variables)
BGB_LIMIT_FILES=3 BGB_SMOKE_TEST=1 python -m src train configs/local/smoke.yaml
# Or use the helper script:
./run_smoke_test.sh

# Full training (watch in tmux recommended)
tmux new -s train
python -m src train configs/local/train.yaml
```

### Modal Cloud Training (A100)
```bash
# Test Mamba CUDA first
modal run deploy/modal/app.py --action test-mamba

# Smoke test (app.py sets BGB_LIMIT_FILES=50 automatically)
modal run --detach deploy/modal/app.py --action train --config configs/modal/smoke.yaml

# Full training (detached)
modal run --detach deploy/modal/app.py --action train --config configs/modal/train.yaml

# Monitor
modal app list
modal app logs <app-id>
```

## üîë Key Configuration Differences

| Setting | Local (RTX 4090) | Modal (A100-80GB) | Why Different |
|---------|------------------|-------------------|---------------|
| **Batch Size** | 8 | 48 | V3 dual-stream uses more memory |
| **Mixed Precision** | false | true | RTX 4090 FP16 causes NaNs |
| **Learning Rate** | 5e-5 | 5e-5 | Reduced for V3 stability |
| **Workers** | 0 | 8 | WSL2 multiprocessing issues |
| **Cache Location** | `cache/tusz/` | `/results/cache/tusz/` | Different filesystems |

## ‚ö†Ô∏è Common Pitfalls

1. **Wrong Cache Directory**:
   - ‚ùå Local: `cache/v2.6_full/` (empty)
   - ‚úÖ Local: `cache/tusz/` (has 3734 files)

2. **Modal Cache Misconception**:
   - ‚ùå "Cache is on S3 causing slowdowns"
   - ‚úÖ Cache is on Modal SSD from first run

3. **Mixed Precision on RTX 4090**:
   - ‚ùå `mixed_precision: true` causes NaN losses
   - ‚úÖ Keep `mixed_precision: false` for stability

4. **PyG Not Installed**:
   - Run `make setup-gpu` locally (installs PyG from prebuilt wheels for torch 2.2.2+cu121)
   - Modal image includes PyG automatically

## üèóÔ∏è Model Configuration (All Configs)

```yaml
model:
  architecture: v3  # V3 dual-stream architecture

  tcn:
    num_layers: 8
    channels: [64, 128, 256, 512]
    kernel_size: 7
    stride_down: 16

  mamba:
    n_layers: 6
    d_model: 512
    d_state: 16
    conv_kernel: 4  # CUDA constraint
    # Node/Edge streams use different params (see detector.py)

  graph:
    enabled: true
    use_pyg: true  # Required for vectorized GNN
    alpha: 0.05    # SSGConv mixing parameter
    k_eigenvectors: 16  # Static Laplacian PE

    # V3-specific edge stream config:
    edge_mamba_layers: 2
    edge_mamba_d_state: 8
    edge_mamba_d_model: 16  # Must be multiple of 8
```

## üìä Expected Training Times

| Config | Platform | Time/Epoch | Total Time | Cost |
|--------|----------|------------|------------|------|
| Local Train | RTX 4090 | ~2-3 hours | ~200-300 hours | Electricity |
| Modal Train | A100-80GB | ~1 hour | ~100 hours | ~$319 |
| Smoke Test | Both | ~5 mins | 5 mins | Minimal |

## üîß Environment Variables

| Variable | Purpose | When to Use |
|----------|---------|-------------|
| `BGB_SMOKE_TEST=1` | Skip seizure sampling | Local smoke tests |
| `BGB_LIMIT_FILES=3` | Limit to 3 files | Local smoke (required!) |
| `BGB_LIMIT_FILES=N` | Limit to N files | Testing |
| `BGB_DISABLE_TQDM=1` | Disable progress bars | Modal (automatic) |
| `BGB_NAN_DEBUG=1` | Debug NaN losses | If training fails |
| `BGB_FORCE_MANIFEST_REBUILD=1` | Rebuild cache manifest | If cache corrupted |

## üìà Post-Processing (All Configs)

```yaml
postprocessing:
  hysteresis:
    tau_on: 0.86   # Seizure onset threshold
    tau_off: 0.78  # Seizure offset threshold
  morphology:
    opening_kernel: 11
    closing_kernel: 31
  duration:
    min_duration_s: 3.0
    max_duration_s: 600.0
```

## üéØ Training Strategy

1. **Focal Loss**: Essential for 12:1 class imbalance
2. **Balanced Sampling**: Ensures seizures in every batch (train.yaml only)
3. **Cosine Schedule**: Smooth learning rate decay
4. **Early Stopping**: Patience=5 on sensitivity@10FA/24h

## üö® Critical Notes

- **V3 Architecture**: Dual-stream with learned edge dynamics
- **BiMamba2 headdim**: Node=8, Edge=4 (prevents CUDA fallback)
- **Cache Reuse**: Both platforms reuse existing preprocessed cache
- **Smoke tests**: Local needs manual env vars, Modal sets automatically
- **Full state-space modeling**: No Conv1d fallbacks with proper headdim

# Modal A100 Dev Set - Hyperparameter Tuning on Cloud
# Use this AFTER training to tune thresholds and hyperparameters

data:
  dataset: tuh_eeg
  data_dir: /data/edf/dev              # Modal S3 mount - DEV SET
  cache_dir: /results/cache/dev        # Modal persistent volume
  sampling_rate: 256
  n_channels: 19
  window_size: 60
  stride: 10
  num_workers: 8                       # A100 can handle more workers
  pin_memory: true
  persistent_workers: true
  validation_split: 0.0                 # No split - use entire dev set
  use_balanced_sampling: true          # CRITICAL: Use positive-aware sampling

preprocessing:
  montage: "10-20"
  bandpass: [0.5, 120]
  notch_freq: 60
  normalize: true
  use_mne: true

model:
  architecture: tcn  # Use TCN instead of U-Net

  tcn:
    num_layers: 8
    channels: [64, 128, 256, 512]
    kernel_size: 7
    dropout: 0.15
    causal: false
    stride_down: 16
    use_cuda_optimizations: true

  mamba:
    n_layers: 6
    d_model: 512
    d_state: 16
    conv_kernel: 4  # CUDA constraint: must be 2-4
    dropout: 0.1

postprocessing:
  hysteresis:
    tau_on: 0.86      # TUNE THESE ON DEV SET
    tau_off: 0.78     # TUNE THESE ON DEV SET
  morphology:
    opening_kernel: 11
    closing_kernel: 31
  duration:
    min_duration_s: 3.0
    max_duration_s: 600.0
  events:
    tau_merge: 2.0
    confidence_method: mean

training:
  epochs: 0                             # NO TRAINING - evaluation only
  batch_size: 128                      # A100 can handle larger batches for inference
  mixed_precision: true

evaluation:
  metrics: [taes, sensitivity, specificity, auroc]
  fa_rates: [10, 5, 2.5, 1]
  mode: tuning                         # Special mode for hyperparameter search

experiment:
  name: modal_dev_tuning_$(date +%Y%m%d_%H%M%S)
  description: "Modal A100 hyperparameter tuning on TUSZ dev set"
  seed: 42
  device: cuda
  output_dir: /results/tuning
  cache_dir: /results/cache/dev
  log_level: INFO
  checkpoint_path: /results/tcn_full_100ep/checkpoints/best.pt  # Load trained TCN model

logging:
  log_every_n_steps: 50
  tensorboard: true
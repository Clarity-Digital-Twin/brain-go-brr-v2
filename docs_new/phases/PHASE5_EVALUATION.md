# PHASE 5 â€” Evaluation, Scoring, and Benchmarking (Ironâ€‘Clad, TDD) âœ… COMPLETE

## ğŸ¯ Phase Goal
Establish a clinically grounded, reproducible evaluation pipeline that turns perâ€‘timestep probabilities into events and reports TAES and sensitivity at target FA/24h operating points on TUH/CHBâ€‘MIT and external benchmarks.

Success means: a deterministic, wellâ€‘tested evaluation stack with clear CLI, CI integration, and artifacts suitable for epilepsybenchmarks.com submission.

## ğŸ”­ Scope & Status âœ… COMPLETE
- âœ… Implementation lives under `src/brain_brr/` (older `src/experiment/*` names are historical)
- âœ… Phase 4 APIs integrated (hysteresis, morphology, stitching)
- âœ… Export functionality in `src/brain_brr/events/export.py` (CSV_BI, JSON)
- âœ… CLI evaluation command in `src/brain_brr/cli/cli.py` (runnable via `python -m src`) with export options
- âœ… Evaluation is CPUâ€‘safe and does not require CUDA

All target APIs defined in this document have been implemented and tested.

## ğŸ“ Metrics (Clinical)
- TAES (Timeâ€‘Aligned Event Scoring): primary temporal alignment score in [0,1].
- Sensitivity@FA: eventâ€‘level sensitivity at FA/24h âˆˆ {10, 5, 2.5, 1}.
- FA curve: sensitivity vs FA/24h curve for multiple targets.
- AUROC (sampleâ€‘level): for sanity and regression tracking.

Additional metrics (implemented):
- âœ… Calibration (ECE) - Expected Calibration Error
- âœ… PRâ€‘AUC - Precision-Recall Area Under Curve
- â³ Perâ€‘patient TAES distribution (future work)

## ğŸ§© Eventization Semantics
Eventization must follow Phase 4: hysteresis (Ï„_on=0.86, Ï„_off=0.78), morphology (openingâ†’closing), min/max duration, and optional stitching (for fullâ€‘record inference). Conventions:
- Thresholding: â‰¥ Ï„_on to enter; < Ï„_off to exit; stable windows min_onset/min_offset when added.
- Morphology kernels are odd; defaults documented in Phase 4.
- Min duration default 3.0s; max duration default 600.0s with segmentation.
- Convert masks to intervals by diff on zeroâ€‘padded mask.

Note: Evaluation currently uses the corrected duration formula for overlapped windows
to compute FA/24h time (for N windows: duration = (Nâˆ’1)Ã—stride + window_size). Full
crossâ€‘window event stitching (merging events crossing window boundaries) is available
in Phase 4 and can be threaded into evaluation as needed.

## ğŸ›ï¸ Threshold Search & FA Targets
- Purpose: choose hysteresis Ï„_on such that FA/24h â‰ˆ target (conservative bisection). Use Ï„_off = max(0, Ï„_on âˆ’ Î”) with Î”â‰ˆ0.08.
- Procedure:
  1) For a candidate Ï„_on, set Ï„_off accordingly and postâ€‘process probs â†’ predicted events.
  2) Compute FA/24h across the corpus using perâ€‘record durations (stitching if needed).
  3) Binary search on Ï„_on âˆˆ [Î”, 1] until tolerance (1eâ€‘4) or max_iters.
- Sensitivity@FA: once Ï„_on found, compute eventâ€‘level sensitivity vs references.

Implementation details:
- The legacy `threshold` parameter in `evaluate.batch_probs_to_events(...)` is deprecated.
  Evaluation selects Ï„_on via binary search to meet the FA/24h target (conservative),
  with Ï„_off = max(0, Ï„_on âˆ’ Î”). Hysteresis values in the config act as defaults/initial
  seeds; evaluation is not limited to fixed config thresholds.
- TAES falseâ€‘alarm penalty weight defaults to Î± = 0.15 in code; this balances temporal
  alignment against spurious predictions.

## ğŸ“š Datasets & Splits
- Train: TUH EEG Seizure Corpus
- Dev/Validation: CHBâ€‘MIT (pediatric generalization)
- Final: epilepsybenchmarks.com

Split discipline:
- Patientâ€‘level splits where applicable.
- Record duration tracked to compute FA/24h accurately.
- Store split manifests for reproducibility (CSV/JSON under `configs/` or `results/manifests/`).

## ğŸ”— Pipeline Overview (Eval Time)
1) Load model outputs (BÃ—T probs) and labels (BÃ—T masks) or run model inference.
2) Postâ€‘process to masks â†’ events (Phase 4 APIs).
3) Threshold search to match FA targets.
4) Compute TAES, sensitivity@FA, FA curve, AUROC.
5) Export artifacts: JSON metrics, CSV_BI events, plots, and threshold table.

## ğŸ§ª TDD Buildâ€‘Out Plan

Unit tests (small, deterministic):
- `tests/test_evaluate.py` (extend):
  - TAES correctness on synthetic cases (single/multiple overlaps, FP penalty).
  - FA/24h computation with controlled durations and gaps.
  - Binary search on Ï„_on converges (monotone FA vs Ï„_on) with mock traces.
  - Sensitivity@FA matches ground truth under simple scenarios.

Integration tests:
- `tests/test_integration_eval.py`:
  - Endâ€‘toâ€‘end: probs â†’ postprocess (Phase 4) â†’ events â†’ metrics (satisfy invariants; e.g., Ï„_on,high â‰¥ Ï„_on,low; FA monotonicity).
  - Window stitching path: reconstruct full timeline and verify boundary behavior.

Export/Compliance tests:
- `tests/test_export.py`:
  - CSV_BI header format, column schema, sorting, duration consistency.

Performance determinism:
- Seed test to assert reproducible metrics across invocations with same inputs.

## ğŸ§© Target APIs (Python)

Already present (to standardize):
- `evaluate.batch_masks_to_events(masks, fs)`
- `evaluate.batch_probs_to_events(probs, post_cfg, fs, threshold)` [Deprecated: `threshold` ignored]
- `evaluate.fa_per_24h(pred_events, ref_events, total_hours)`
- `evaluate.calculate_taes(pred_events, ref_events, alpha=0.15)`
- `evaluate.find_threshold_for_fa_eventized(probs, post_cfg, ref_events, fa_target, total_hours, fs)`
- `evaluate.sensitivity_at_fa_rates(probs, labels, fa_targets, post_cfg, sampling_rate)`
- `evaluate.evaluate_predictions(probs, labels, fa_rates, post_cfg, sampling_rate)`

Phase 4 dependencies (implemented):
- `postprocess.apply_hysteresis`, `apply_morphology`, `filter_duration`, `stitch_windows`
- `events.mask_to_events`, `events.merge_events`, `events.calculate_event_confidence`
- `export.export_csv_bi`

During Phase 5, swap in the Phase 4 APIs internally (keep current functions as adapters for test stability until migration is complete).

Outputs enhancement:
- `evaluate.evaluate_predictions(...)` returns a `thresholds` table mapping FA target â†’ Ï„_on
  used for sensitivity evaluation (e.g., `{ "10": 0.8725, "5": 0.9031, ... }`).

## ğŸ§µ CLI & Make Targets

CLI examples:
- Evaluate (dev tuning):
  `python -m src evaluate /path/to/checkpoint.pt data_ext4/tusz/edf/dev --config configs/tusz_dev_tuning.yaml --output-json results/dev_metrics.json`
- Final eval:
  `python -m src evaluate /path/to/checkpoint.pt data_ext4/tusz/edf/eval --config configs/tusz_eval_final.yaml --output-json results/final_metrics.json`

Exports: use library functions in `src/brain_brr/events/export.py` (CSV_BI/JSON).

Make (optional additions):
- `make eval-dev` â†’ run evaluation on validation split; write metrics JSON + plots.
- `make export-dev` â†’ export CSV_BI for dev split.

## ğŸ“¦ Artifacts & Logging
- Metrics JSON: TAES, AUROC, sensitivity@{10,5,2.5,1}fa, FA curve.
- Threshold table: FA target â†’ Ï„_on.
- CSV_BI folder with perâ€‘record events.
- Plots: FA curve, ROC, calibration (optional).
- Manifest: data split listing, duration totals.
- Metadata: config hash, git commit, timestamp, env (CUDA, torch, mamba_ssm present/absent).

## ğŸ” Reproducibility & Determinism
- Set torch/np/python seeds and deterministic flags.
- Cache keys based on config hash (noted in AGENTS.md critical notes).
- For CI CPU runs where gpu extra may be installed, set `SEIZURE_MAMBA_FORCE_FALLBACK=1` to avoid GPU path.

## âœ… Definition of Done (Phase 5)
- Unit + integration tests passing; coverage â‰¥ 90% for evaluation modules.
- `make q` passes (ruff + mypy strict) after final refactor.
- CLI evaluates dev split and writes complete artifacts.
- CSV_BI exporter matches Temple format; spotâ€‘checked in tests.
- Documentation (this file + README link) upâ€‘toâ€‘date and concise.

## âš ï¸ Risks & Mitigations
- Nonâ€‘monotone FA vs Ï„_on under rare postâ€‘proc combos â†’ enforce monotonicity by increasing Ï„_on when binary search oscillates; log warning.
- Stitching boundary artifacts â†’ tests with partial windows and offâ€‘byâ€‘one checks.
- Dataset header quirks (TUH EDF) â†’ addressed in `TUSZ_EDF_HEADER_FIX.md`; ensure eval respects fixed channel order.
- CPU/GPU divergence â†’ Evaluation runs CPUâ€‘deterministic; GPU only affects model inference (Phase 2/3).

## ğŸ“Œ References
- NEDC TAES specifications and scoring methodology.
- epilepsybenchmarks.com submission formats.
- Internal Phase docs: PHASE2.* (model), PHASE3_TRAINING_PIPELINE.md, PHASE4_POSTPROCESSING.md.
> Note: This Phase doc is being replaced by componentâ€‘oriented docs. See components/evaluation.md for the canonical, codeâ€‘aligned reference.

# S3 Cache Integration Plan

## Current State
- Modal smoke test IS building cache now (see logs at 22:49)
- Directories auto-create via `mkdir(parents=True, exist_ok=True)`
- Cache builds from raw S3 data if not found locally

## Proposed S3 Cache Integration

### Step 1: After Local Cache Completes
```bash
# Upload to S3 with cache prefix
aws s3 sync cache/tusz/ s3://brain-go-brr-eeg-data-20250919/cache/tusz/
```

### Step 2: Add S3 Cache Mount to Modal
```python
# In deploy/modal/app.py, add:
cache_mount = modal.CloudBucketMount(
    "brain-go-brr-eeg-data-20250919",
    secret=s3_secret,
    key_prefix="cache/tusz/",
    read_only=True,
)
```

### Step 3: Smart Cache Check Logic
```python
# Pseudo-code for cache priority:
def get_cache_source():
    # 1. Check Modal persistent volume
    if Path("/results/cache/tusz/manifest.json").exists():
        if is_valid_cache("/results/cache/tusz"):
            return "modal_volume"

    # 2. Check S3-mounted cache
    if Path("/s3-cache/manifest.json").exists():
        if is_valid_cache("/s3-cache"):
            # Copy from S3 to Modal volume for faster subsequent access
            shutil.copytree("/s3-cache", "/results/cache/tusz")
            return "s3_copied"

    # 3. Build from scratch
    return "build_fresh"
```

## Benefits
1. **First run after S3 upload**: Uses S3 cache (saves 2-3 hours)
2. **Subsequent runs**: Uses Modal volume cache (fastest)
3. **Smoke tests**: Can use same S3 cache (just limits files via BGB_LIMIT_FILES)

## Implementation Priority
1. âœ… Let current smoke test finish building cache
2. âœ… Verify cache is valid with metadata
3. ðŸ”„ Upload to S3 when local build completes
4. ðŸ”„ Add S3 mount to Modal app
5. ðŸ”„ Test with full training

## Cost Savings
- Cache build time: ~2-3 hours @ $3.19/hr = ~$10
- S3 storage: ~50GB @ $0.023/GB/month = ~$1.15/month
- S3 â†’ Modal transfer: FREE (same region)
- **ROI: Saves $10 per training run**
============================= test session starts ==============================
platform linux -- Python 3.11.13, pytest-8.4.2, pluggy-1.6.0
rootdir: /mnt/c/Users/JJ/Desktop/Clarity-Digital-Twin/brain-go-brr-v2
configfile: pyproject.toml
testpaths: tests
plugins: anyio-4.10.0, cov-7.0.0, mock-3.15.1, sugar-1.1.1, xdist-3.8.0
collected 112 items

tests/test_data.py .......                                               [  6%]
tests/test_decoder.py ..............                                     [ 18%]
tests/test_encoder.py ...............                                    [ 32%]
tests/test_evaluate.py ................                                  [ 46%]
tests/test_full_model.py .........                                       [ 54%]
tests/test_mamba.py .......s.............                                [ 73%]
tests/test_rescnn.py ...................                                 [ 90%]
tests/test_smoke.py .                                                    [ 91%]
tests/test_training.py ..........                                        [100%]

=============================== warnings summary ===============================
src/experiment/models.py:298
  /mnt/c/Users/JJ/Desktop/Clarity-Digital-Twin/brain-go-brr-v2/src/experiment/models.py:298: UserWarning: mamba-ssm not available; using Conv1d fallback for BiMamba2. Fallback is for shape validation only and is not functionally equivalent.
    warnings.warn(

tests/test_full_model.py: 9 warnings
tests/test_mamba.py: 20 warnings
tests/test_training.py: 8 warnings
  /mnt/c/Users/JJ/Desktop/Clarity-Digital-Twin/brain-go-brr-v2/src/experiment/models.py:341: UserWarning: Using Conv1d fallback for BiMamba2Layer â€” NOT equivalent to Mamba-2.
    warnings.warn(

tests/test_training.py::TestTrainingSmoke::test_scheduler_creation
tests/test_training.py::TestTrainingSmoke::test_full_training_loop
  /mnt/c/Users/JJ/Desktop/Clarity-Digital-Twin/brain-go-brr-v2/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
_______________ coverage: platform linux, python 3.11.13-final-0 _______________

Name                         Stmts   Miss  Cover   Missing
----------------------------------------------------------
src/__main__.py                  1      1     0%   3
src/cli.py                     156    156     0%   3-308
src/experiment/data.py         170     40    76%   53-55, 105, 117-119, 130-136, 191-195, 225, 246, 276, 285, 291-293, 297-300, 306, 318-319, 336-344, 352, 355-356
src/experiment/evaluate.py     146      8    95%   51, 62, 104, 201-204, 363-365
src/experiment/models.py       223     12    95%   109, 198, 295, 329-332, 366, 374, 449, 567-581, 590, 707
src/experiment/pipeline.py     234     40    83%   54-56, 79, 110, 146, 204, 290, 388, 490-491, 541-542, 587-664
src/experiment/schemas.py      175     38    78%   45, 67-72, 89-91, 105-110, 150, 254, 290, 295, 300-309, 313-326
----------------------------------------------------------
TOTAL                         1114    295    74%

3 files skipped due to complete coverage.
============ 111 passed, 1 skipped, 40 warnings in 78.42s (0:01:18) ============

# ðŸš¨ Modal A100 Performance Fix - Critical Issue Resolved

## Problem Identified
Modal training appeared stuck at batch 775/778 for **HOURS** (from 4:20am to 5:46am+)

## Root Cause Analysis

### 1. **Not Actually Stuck - It's Validation!**
- Training completed all 778 batches successfully
- Entered validation phase which has **810 batches** (more than training!)
- No progress output during validation with `BGB_DISABLE_TQDM=1`
- Validation was running but appeared frozen

### 2. **CPU Bottleneck During Validation**
- Modal default: 0.125 CPU cores (ridiculously low!)
- Previous config: 8 CPU cores (better but still limiting)
- With `num_workers: 8` in config, each worker fights for CPU time
- Validation dataset loading became CPU-bound

### 3. **Memory Pressure**
- Validation loads 51,901 windows vs training's 49,760
- Previous: 32GB RAM might cause swapping
- GNN+LPE adds memory overhead for graph computations

## Solution Applied

### Updated Modal Function Configuration:
```python
@app.function(
    gpu="A100-80GB",
    timeout=86400,  # 24 hours
    memory=65536,   # Increased to 64GB (was 32GB)
    cpu=16,         # Increased to 16 cores (was 8)
)
```

### Why This Fixes It:
1. **16 CPU cores**: Supports 8 DataLoader workers properly (2 cores per worker)
2. **64GB RAM**: Handles larger validation dataset without swapping
3. **Prevents bottleneck**: CPU no longer constrains GPU utilization

## Performance Impact

### Before Fix:
- Training: ~48s/batch (CPU starved)
- Validation: Appears stuck (CPU bottleneck)
- Epoch time: Unknown (never completed!)

### After Fix (Expected):
- Training: ~5s/batch with proper CPU
- Validation: Should complete normally
- Epoch time: ~1 hour total

## Modal 2025 CPU Options

### Available Configurations:
- **Default**: 0.125 cores (DO NOT USE!)
- **Custom**: Up to 96+ cores available
- **Recommended for A100**: 16-32 cores
- **Cost**: $0.192/core/hour (minimal compared to GPU)

### GPU-Specific Recommendations:
| GPU | Recommended CPUs | Memory | Use Case |
|-----|-----------------|---------|----------|
| A100-80GB | 16-32 cores | 64-128GB | Large models, multi-worker loading |
| H100 | 32-64 cores | 128-256GB | Massive models, distributed training |
| A10G | 8-16 cores | 32-64GB | Medium workloads |
| T4 | 4-8 cores | 16-32GB | Light inference |

## Lessons Learned

1. **Always allocate sufficient CPUs**: GPU training still needs CPU for data loading
2. **Monitor validation separately**: It can have different characteristics than training
3. **Validation can be larger**: Don't assume validation < training dataset size
4. **Add progress logging**: Silent validation looks like hanging

## Configuration Best Practices

### For A100 Training with DataLoaders:
```python
@app.function(
    gpu="A100-80GB",
    cpu=16,  # 2 cores per DataLoader worker
    memory=65536,  # 64GB for large datasets
)
```

### Config YAML:
```yaml
data:
  num_workers: 8  # Should have 2+ CPU cores per worker
  pin_memory: true
  persistent_workers: true
```

## Action Items
- [x] Increase CPU allocation to 16 cores
- [x] Increase memory to 64GB
- [x] Document findings
- [ ] Add validation heartbeat logging (already in code)
- [ ] Test with new configuration
- [ ] Monitor CPU utilization during next run

## Cost Analysis
- Additional 8 CPUs: +$1.54/hour
- Additional 32GB RAM: +$0.77/hour
- **Total increase**: ~$2.31/hour
- **Worth it**: Prevents multi-hour stalls!

## Bottom Line
The "stuck" training was actually running validation with severe CPU bottlenecks. Increasing CPU cores from 8â†’16 and RAM from 32GBâ†’64GB should resolve the issue completely.
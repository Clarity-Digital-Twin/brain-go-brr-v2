# ðŸš€ Future Roadmap: Next-Generation EEG Architecture Stack

## Executive Summary

Moving beyond the current U-Net + ResCNN + Bi-Mamba-2 architecture to address fundamental limitations in EEG seizure detection, particularly the montage-dependency problem and multi-scale temporal modeling.

**Proposed Stack (EvoBrainâ€‘aligned)**: Biâ€‘Mamba (time) â†’ Dynamic GNN (graph) â†’ ConvNeXt â†’ optional TCN

---

## Current Architecture (v2)

```
EEG (19ch, 256Hz) â†’ U-Net â†’ ResCNN â†’ Bi-Mamba-2 â†’ Detection Head
```

### Limitations
- **Montage-dependent**: Assumes fixed electrode relationships
- **U-Net overhead**: Complex skip connections for temporal data
- **ResNet aging**: Newer architectures outperform vanilla ResNets
- **No spatial reasoning**: Treats channels as independent rather than spatially related

---

## Proposed Experimental Stack (v3)

```
EEG (19ch, 256 Hz, 60 s)
  â†“
Biâ€‘Mambaâ€‘2 (time encoder)
  â†“
Dynamic GCN + Laplacian PE (graph encoder; explicit A_t)
  â†“
ConvNeXt (local refinement)
  â†“
Optional TCN (extra multiâ€‘scale)
  â†“
Detection Head
```

---

## Component Rationale

### 1. Dynamic Graph Neural Network (GNN) â€” Spatial Reasoning
**Problem Solved**: Montage dependency
- Fixed adjacency is insufficient; edges must evolve with state (explicit dynamics)
- Use GCN with Laplacian Positional Encoding (topâ€‘K eigenvectors)
- Build timeâ€‘varying adjacency A_t from temporal features (cosine or learned)
- Input: perâ€‘channel embeddings from Biâ€‘Mambaâ€‘2
- Output: spatially aware features per time step

### 2. Biâ€‘Mambaâ€‘2 â€” Temporal Encoder
Already present in v2; becomes the first stage in timeâ€‘thenâ€‘graph pipeline per EvoBrain.

### 3. ConvNeXt - Local Pattern Enhancement
**Problem Solved**: Outdated ResNet blocks
- **Current issue**: ResNet from 2015, surpassed by modern designs
- **ConvNeXt solution**:
  - Larger kernels (7Ã—1 or 9Ã—1) for longer temporal patterns
  - Fewer activation functions
  - Layer normalization > Batch normalization
  - Inverted bottleneck design
- **Performance**: Matches Vision Transformers with pure convolution

### 4. Optional TCN â€” Multiâ€‘Scale Addâ€‘on
Add only if multiâ€‘scale detail is lacking after graph integration; keep latency budget in mind.

---

## Implementation Phases

### Phase 1: Baseline Ablation (Months 1-2)
- [ ] Benchmark current U-Net + ResCNN + Bi-Mamba on TUSZ
- [ ] Document baseline metrics (TAES at 10/5/1 FA/24h)
- [ ] Profile computational requirements

### Phase 2: Add Dynamic GNN after Biâ€‘Mamba (Months 2â€“3)
- [ ] Implement GraphChannelMixer (GCN+LPE) module gated by config
- [ ] Adjacency A_t from temporal features (cosine similarity / learned)
- [ ] Unit tests (shape, identity init) and integration tests

### Phase 3: ConvNeXt Integration (Months 3-4)
- [ ] Replace ResCNN with ConvNeXt blocks
- [ ] Tune: Kernel sizes (7Ã—1, 9Ã—1, 11Ã—1)
- [ ] Compare: Local pattern detection quality
- [ ] Expected: 5-10% performance improvement

### Phase 4: Dualâ€‘Stream Temporal (Months 4â€“6)
- [ ] Prototype nodeâ€‘stream and edgeâ€‘stream Mamba (EvoBrain)
- [ ] Edge stream supervises/builds A_t; node stream feeds GCN
- [ ] Montage generalization experiments (referential â†” bipolar)

### Phase 5: Full Stack Optimization (Months 6-7)
- [ ] Joint training of all components
- [ ] Hyperparameter search
- [ ] Multi-objective optimization (sensitivity vs FA rate)

---

## Evaluation Strategy

### Datasets
1. **Primary**: TUSZ v2.0.3 (train/dev/eval splits)
2. **Transfer**: CHB-MIT (pediatric, different montage)
3. **Stress test**: Siena Scalp EEG (different equipment)

### Metrics
- **Primary**: NEDC TAES scoring (proper clinical evaluation)
- **Sensitivity targets**: >75% at 1 FA/24h, >90% at 5 FA/24h
- **Montage robustness**: Performance delta between montages <10%
- **Computational**: Must maintain real-time capability (<100ms per 60s window)

### Ablation Studies
1. **Component contribution**: Remove each component, measure delta
2. **Order sensitivity**: Try different component orders
3. **Parallel vs Sequential**: Test parallel branches
4. **Depth tuning**: Optimal layers per component

---

## Expected Challenges

### Technical
- **Training complexity**: 4 components = harder optimization
- **Memory requirements**: GNN adds spatial dimension
- **Hyperparameter explosion**: Each component has its own settings
- **Gradient flow**: Ensuring gradients reach all components

### Scientific
- **Overfitting risk**: More parameters with same data
- **Interpretability**: GNN attention weights need validation
- **Generalization**: Does montage-agnostic really work?

### Practical
- **Compute requirements**: Full grid search infeasible
- **Implementation time**: 6-7 month timeline
- **Baseline comparison**: Need strong v2 baseline first

---

## Success Criteria

### Minimum Viable Improvement
- 10% reduction in FA rate at same sensitivity
- OR 10% sensitivity improvement at same FA rate
- Successful transfer to different montage without retraining

### Stretch Goals
- Sub-1 FA/24h with >80% sensitivity
- Real-time inference on CPU
- Interpretable electrode importance maps via GNN attention

---

## Alternative Architectures to Consider

### Hybrid Approaches
```
Option A: Parallel Processing
EEG â†’ [GNN, TCN] â†’ Concat â†’ ConvNeXt â†’ Bi-Mamba

Option B: Multi-Head
EEG â†’ GNN â†’ [TCN, ConvNeXt, Attention] â†’ Fusion â†’ Bi-Mamba

Option C: Hierarchical
EEG â†’ GNN â†’ TCN â†’ [ConvNeXt + Bi-Mamba parallel] â†’ Fusion
```

### Simpler Alternatives (if full stack fails)
1. Just TCN + Bi-Mamba (drop GNN and ConvNeXt)
2. GNN + existing U-Net + Bi-Mamba (only add spatial)
3. Current stack + GNN postprocessing (minimal change)

---

## Key Innovations

1. **First GNN + SSM combination for EEG** (novel to literature)
2. **Montage-agnostic by design** (solves fundamental problem)
3. **Principled multi-scale architecture** (each component has clear role)
4. **O(N) complexity maintained** (scalable to continuous monitoring)

---

## Next Steps

1. [ ] Finish v2 benchmarking (current architecture)
2. [ ] Literature review on GNNs for EEG (identify best practices)
3. [ ] Implement TCN baseline (simplest component to start)
4. [ ] Collaborate/discuss with signal processing experts
5. [ ] Secure compute resources for extensive ablations

---

## References

- TCN: "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling" (2018)
- ConvNeXt: "A ConvNet for the 2020s" (2022)
- GNN for EEG: "EEG-GCNN: Augmenting Electroencephalogram-based Neurological Disease Diagnosis using a Domain-guided Graph Convolutional Neural Network" (2020)
- Mamba: "Mamba: Linear-Time Sequence Modeling with Selective State Spaces" (2024)

---

*Note: This is a research roadmap for experimental architecture. Current v2 (U-Net + ResCNN + Bi-Mamba) remains the primary focus until benchmarked.*

**Last Updated**: 2025-09-21
**Status**: Proposed - Pending v2 Baseline Completion

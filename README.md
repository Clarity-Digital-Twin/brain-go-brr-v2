# ğŸ§  Brain-Go-Brr v2: Next-Generation Seizure Detection with Bi-Mamba State Space Models

## ğŸ¯ Mission Statement 
 
This project pioneers the first systematic evaluation of **bidirectional Mamba-2 state space models** combined with **U-Net CNN encoders** and **Residual CNN stacks** for clinical-grade seizure detection. While transformers have dominated recent EEG research, their O(NÂ²) complexity limits real-time deployment. We propose a novel architecture achieving O(N) complexity while maintaining or exceeding transformer performance.

**No one has tested this specific architecture combination before.**

## Why This Matters

Current seizure detection systems suffer from:
- **High false alarm rates** (>10 FA/24h) making them clinically unusable
- **Poor cross-dataset generalization** limiting real-world deployment
- **Computational complexity** preventing real-time edge deployment
- **Lack of standardized evaluation** making progress assessment difficult

We address all four challenges with a unified approach.

## Novel Architecture: U-Net + ResCNN + Bi-Mamba-2

```
EEG Input (19ch, 256Hz)
    â†“
[U-Net Encoder (1D CNN)]  â† Multi-scale morphology extraction
    â†“
[ResCNN Stack (3 blocks)] â† Local pattern enhancement
    â†“
[Bi-Mamba-2 (6 layers)]   â† Long-range temporal dependencies (O(N))
    â†“
[U-Net Decoder]           â† Multi-resolution reconstruction
    â†“
[Sigmoid + Hysteresis]    â† Dual-threshold (Ï„_on=0.86, Ï„_off=0.78)
    â†“
[TAES Scoring]            â† Time-aligned clinical evaluation
```

### Key Innovations

1. **First Bi-Mamba-2 for Seizure Detection**: Replacing transformers with state space models for linear complexity
2. **Unified Multi-Scale Architecture**: Combining CNN locality with SSM global context
3. **Hysteresis Post-Processing**: Clinically-inspired dual thresholds reducing false alarms
4. **TAES-First Evaluation**: Focusing on Time-Aligned Event Scoring for clinical relevance

## Performance Targets

Based on NEDC benchmarking standards and epilepsybenchmarks.com:

| Metric | Target | Current SOTA | Why It Matters |
|--------|--------|--------------|----------------|
| Sensitivity @ 10 FA/24h | >95% | ~90% | Clinical usability threshold |
| Sensitivity @ 5 FA/24h | >90% | ~82% | ICU deployment standard |
| Sensitivity @ 1 FA/24h | >75% | ~65% | Home monitoring goal |
| TAES Score | >0.85 | ~0.75 | Temporal alignment quality |
| Cross-dataset AUC | >0.95 | ~0.92 | Generalization capability |

## ğŸ› ï¸ Technical Stack (2025 Best Practices)

- **Python 3.11+** with UV package manager (10-100x faster than pip)
- **PyTorch 2.5+** with `torch.compile` (FlashAttention optional in future extras)
- **MNE-Python** for robust EDF/BDF file I/O and montage handling
- **Ruff** for blazing-fast linting/formatting (replacing Black/isort/flake8)
- **Pre-commit hooks** ensuring code quality
- **Apache 2.0 License** for open collaboration

### Mamba CUDA Dispatch

- Default temporal conv kernel is `d_conv=5` (docs/configs). The CUDA kernel in `mamba-ssm` supports widths {2,3,4}.
- We keep `5` publicly and internally coerce to `4` for the CUDA path only. CPU fallback (Conv1d) uses the configured kernel.
- Real Mamba runs only if `mamba-ssm` is importable, CUDA is available, and tensors are on GPU.
- Force fallback regardless of CUDA by exporting `SEIZURE_MAMBA_FORCE_FALLBACK=1`.

## ğŸš€ Quick Start

```bash
# Install UV (modern Python package manager)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Setup project
make setup

# Optional: install extras
#   - GPU/SSM: Mamba-SSM kernels
#   - Post-processing: SciPy ndimage operations
#   - Evaluation: pandas (CSV_BI export)
# uv sync -E gpu,post,eval

# Run quality checks (lint + type checking)
make q  # Run after every change!

# Run local training (reduced data)
make train-local

# Full training with wandb logging
make train
```

### ğŸŒ©ï¸ Cloud Deployment (Modal.com)

```bash
# Install Modal CLI
pip install --upgrade modal

# Authenticate
modal setup

# Run training on L40S GPU (48GB VRAM)
modal run modal_train.py --action train --config configs/smoke_test.yaml

# Large-scale training on 8x H100 GPUs
modal run modal_train.py --action train-large --config configs/full_training.yaml
```

See [docs/deployment/MODAL_DEPLOYMENT_GUIDE.md](docs/deployment/MODAL_DEPLOYMENT_GUIDE.md) for complete cloud deployment instructions.

Note for WSL/cross-filesystem users âš™ï¸
- If you ever see a uv hardlink warning during installs, it's harmless. For silence and stability, you can export these (already defaulted in the Makefile):
  - `export UV_LINK_MODE=copy`
  - `export UV_CACHE_DIR=.uv_cache`

## ğŸ“š Literature Foundation

Our approach synthesizes insights from key papers:

- **FEMBA (2024)**: 21,000-hour pre-trained bidirectional Mamba for EEG
- **EEGMamba (2024)**: Mixture of Experts SSM achieving 98.5% on CHB-MIT
- **SeizureTransformer (2022)**: U-Net + ResCNN baseline architecture
- **NEDC/TAES (2021)**: Clinical evaluation metrics and scoring methodology

## Preprocessing Pipeline

MNE-hybrid approach optimized for seizure morphology:

1. **Robust EDF reading** with MNE's battle-tested parser
2. **19-channel 10-20 montage** standardization
3. **Bandpass filtering** (0.5-120 Hz) preserving seizure signatures
4. **60 Hz notch filter** removing line noise
5. **Sliding windows** (60s, 10s stride) for temporal context

## Model Architecture Details

### U-Net Encoder (1D CNN)
- 4 stages: [64, 128, 256, 512] channels
- Kernel size 5, stride 2 downsampling
- Captures multi-scale EEG morphology

### ResCNN Stack
- 3 residual blocks at bottleneck
- Multi-kernel [3, 5, 7] for frequency diversity
- Dropout 0.1 for regularization

### Bi-Mamba-2 Core
- 6 bidirectional layers
- d_model=512, d_state=16
- Selective state spaces with hardware-aware implementation
- O(N) complexity vs Transformer's O(NÂ²)

### Post-Processing
- Hysteresis: Ï„_on=0.86, Ï„_off=0.78 with stability windows (min_onset=128, min_offset=256 samples)
- Morphology: opening â†’ closing with odd kernels (defaults: 11 and 31 samples)
- Duration filter: keep 3s â‰¤ duration â‰¤ 600s; segment longer events
- Stitching: overlap-add (uniform/weighted) and max options
- Confidence: mean/peak/percentile per event, clamped to [0,1]

GPU parity: morphology uses pooling (max/min) on CUDA; CPU path uses SciPy ndimage. See [docs/phases/PHASE4_POSTPROCESSING.md](docs/phases/PHASE4_POSTPROCESSING.md) for details.

### Evaluation
- Time-Aligned Event Scoring (TAES), Sensitivity@FA/24h, FA curve, AUROC
- Threshold search varies hysteresis Ï„_on (with Ï„_off = Ï„_on âˆ’ 0.08) to hit FA targets {10, 5, 2.5, 1}
- FA/24h time uses overlap-aware duration: (Nâˆ’1)Ã—stride + window_size

See [docs/phases/PHASE5_EVALUATION.md](docs/phases/PHASE5_EVALUATION.md) for the end-to-end evaluation and benchmarking plan.
For online/real-time inference, see [docs/phases/PHASE6_STREAMING.md](docs/phases/PHASE6_STREAMING.md).

## ğŸ“Š Evaluation Strategy

### Primary Metrics (NEDC Standard)
- TAES @ [10, 5, 2.5, 1] FA/24h
- Sensitivity/Specificity curves
- Cross-dataset generalization

### Benchmark Datasets
1. **TUH EEG Seizure Corpus** (primary)
2. **CHB-MIT** (pediatric validation)
3. **epilepsybenchmarks.com** (final evaluation)

## ğŸ—‚ï¸ Project Structure

```
brain-go-brr-v2/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ brain_brr/        # Core modules (refactored from experiment/)
â”‚       â”œâ”€â”€ models/       # Neural network components
â”‚       â”‚   â”œâ”€â”€ detector.py  # Main SeizureDetector class
â”‚       â”‚   â”œâ”€â”€ unet.py      # U-Net encoder/decoder
â”‚       â”‚   â”œâ”€â”€ rescnn.py    # Residual CNN blocks
â”‚       â”‚   â””â”€â”€ mamba.py     # Bidirectional Mamba-2
â”‚       â”œâ”€â”€ data/         # EEG preprocessing & datasets
â”‚       â”‚   â”œâ”€â”€ io.py        # EDF/MNE file handling
â”‚       â”‚   â”œâ”€â”€ dataset.py   # PyTorch Dataset/DataLoader
â”‚       â”‚   â””â”€â”€ preprocess.py # Filtering, windowing, normalization
â”‚       â”œâ”€â”€ train/        # Training pipeline
â”‚       â”‚   â”œâ”€â”€ trainer.py   # PyTorch Lightning trainer
â”‚       â”‚   â””â”€â”€ callbacks.py # Checkpointing, early stopping
â”‚       â”œâ”€â”€ post/         # Post-processing
â”‚       â”‚   â”œâ”€â”€ postprocess.py # Hysteresis, morphology, stitching
â”‚       â”‚   â””â”€â”€ events.py     # Event generation, confidence
â”‚       â”œâ”€â”€ eval/         # Evaluation metrics
â”‚       â”‚   â”œâ”€â”€ metrics.py    # TAES, AUROC, FA curves
â”‚       â”‚   â””â”€â”€ benchmark.py  # epilepsybenchmarks.com adapter
â”‚       â”œâ”€â”€ config/       # Configuration schemas
â”‚       â”‚   â””â”€â”€ schemas.py    # Pydantic v2 config models
â”‚       â”œâ”€â”€ cli/          # Command-line interface
â”‚       â”‚   â””â”€â”€ cli.py        # Main entry point
â”‚       â””â”€â”€ utils/        # Shared utilities
â”‚           â””â”€â”€ pick_utils.py # Channel selection helpers
â”œâ”€â”€ configs/              # YAML experiment configs (EEG-focused)
â”œâ”€â”€ docs/                 # Documentation
â”‚   â”œâ”€â”€ architecture/     # Architecture specs
â”‚   â”œâ”€â”€ deployment/       # Modal.com deployment guide
â”‚   â””â”€â”€ phases/           # Development phases (1-6)
â”œâ”€â”€ tests/                # Pytest test suites
â”œâ”€â”€ modal_train.py        # Modal cloud deployment
â””â”€â”€ Makefile              # Automation commands
```

## ğŸ—ï¸ Architecture Benefits

The modular refactoring from monolithic `src/experiment/` to `src/brain_brr/` provides:

1. **Clear Separation of Concerns**: Each module has a single responsibility
2. **Easier Testing**: Isolated components with focused test coverage
3. **Better Maintainability**: Navigate directly to functionality
4. **Cloud-Ready**: Modal.com deployment script included
5. **Extensibility**: Easy to add new models, metrics, or preprocessing

## Key Development Notes

- **Quality First**: Run `make q` after every change for lint + type checking
- **Type Everything**: Full type hints required throughout codebase
- **Test Coverage**: Unit and integration tests with pytest markers
- **No Comments**: Code should be self-documenting (unless explicitly needed)
- **Follow Patterns**: Match style of neighboring files

## Why This Will Work

1. **Proven Components**: Each piece (U-Net, ResCNN, Mamba) has shown success individually
2. **Novel Combination**: First to combine all three for seizure detection
3. **Clinical Focus**: TAES-first design prioritizing false alarm reduction
4. **Modern Engineering**: 2025 best practices ensuring reproducibility
5. **Open Science**: Apache 2.0 license enabling collaboration

## Citation

If you use this code, please cite:

```bibtex
@software{brain-go-brr-v2,
  title={Brain-Go-Brr v2: Bi-Mamba State Space Models for Seizure Detection},
  author={Clarity Digital Twin Project},
  year={2025},
  url={https://github.com/clarity-digital-twin/brain-go-brr-v2},
  license={Apache-2.0}
}
```

## âœ‰ï¸ Contact

For questions, issues, or collaboration:
- GitHub Issues: [Create an issue](https://github.com/clarity-digital-twin/brain-go-brr-v2/issues)
- Email: [Contact maintainers]

---

*"Reducing false alarms from 100+ to <1 per day will transform epilepsy care."*
